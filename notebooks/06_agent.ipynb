{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a94a26-d6fe-408b-a891-dde3b121dcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION zg.production_scheduling_demo.down_machines_uc()\n",
    "RETURNS TABLE (\n",
    "  machine_id STRING,\n",
    "  machine_name STRING,\n",
    "  status STRING,\n",
    "  reported_at TIMESTAMP\n",
    ")\n",
    "RETURN\n",
    "WITH latest AS (\n",
    "  SELECT\n",
    "    machine_id,\n",
    "    machine_name,\n",
    "    status,\n",
    "    reported_at,\n",
    "    ROW_NUMBER() OVER (PARTITION BY machine_id ORDER BY reported_at DESC) AS rn\n",
    "  FROM zg.production_scheduling_demo.machine_status\n",
    ")\n",
    "SELECT machine_id, machine_name, status, reported_at\n",
    "FROM latest\n",
    "WHERE rn = 1\n",
    "  AND status = 'down';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a994d9f6-2732-49ed-a31e-40daabe97c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION zg.production_scheduling_demo.optimize_routes_and_calculate_kpis(\n",
    "  down_machines ARRAY<STRING>,\n",
    "  planning_days INT,\n",
    "  commit BOOLEAN\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  order_id STRING,\n",
    "  machine_id STRING,\n",
    "  profit DOUBLE,\n",
    "  p_best DOUBLE,\n",
    "  processing_hours DOUBLE,\n",
    "  profit_score DOUBLE,\n",
    "  base_confidence DOUBLE,\n",
    "  expected_profit DOUBLE,\n",
    "  expected_ontime_deliveries DOUBLE,\n",
    "  factory_capacity_utilization DOUBLE,\n",
    "  expected_orders_completed INT\n",
    ")\n",
    "LANGUAGE PYTHON\n",
    "COMMENT \"Runs greedy assignment and KPI scoring in memory. Commits to UC only if commit=true.\"\n",
    "HANDLER \"OptimizerHandler\"\n",
    "AS $$\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class OptimizerHandler:\n",
    "    def __init__(self):\n",
    "        self.catalog = \"zg\"\n",
    "        self.schema = \"production_scheduling_demo\"\n",
    "\n",
    "    def __call__(self, down_machines, planning_days, commit):\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        cand = spark.table(f\"{self.catalog}.{self.schema}.candidate_routes_scored\").toPandas()\n",
    "        mach = spark.table(f\"{self.catalog}.{self.schema}.machines_catalog\").toPandas()\n",
    "        down_machines = set(down_machines or [])\n",
    "\n",
    "        # --- Greedy assignment logic ---\n",
    "        machine_caps = {\n",
    "            m[\"machine_id\"]: float(m[\"daily_capacity_hours\"] * planning_days)\n",
    "            for _, m in mach.iterrows()\n",
    "        }\n",
    "        for m in down_machines:\n",
    "            machine_caps[m] = 0.0\n",
    "\n",
    "        order_priority = (\n",
    "            cand.groupby(\"order_id\")\n",
    "                .agg(max_profit_score=(\"profit_score\", \"max\"),\n",
    "                     min_due=(\"promised_date\", \"min\"),\n",
    "                     min_hours=(\"processing_hours\", \"min\"))\n",
    "                .sort_values(by=[\"max_profit_score\", \"min_due\", \"min_hours\"],\n",
    "                             ascending=[False, True, True])\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "        cand_by_order = {\n",
    "            oid: g.sort_values(by=[\"profit_score\", \"processing_hours\"], ascending=[False, True])\n",
    "            for oid, g in cand.groupby(\"order_id\", sort=False)\n",
    "        }\n",
    "\n",
    "        assigned = []\n",
    "        for _, rowp in order_priority.iterrows():\n",
    "            oid = rowp[\"order_id\"]\n",
    "            group = cand_by_order[oid]\n",
    "            for _, c in group.iterrows():\n",
    "                mid = c[\"machine_id\"]\n",
    "                if mid in down_machines:\n",
    "                    continue\n",
    "                need = float(c[\"processing_hours\"])\n",
    "                if machine_caps.get(mid, 0.0) >= need:\n",
    "                    machine_caps[mid] -= need\n",
    "                    assigned.append({\n",
    "                        \"order_id\": oid,\n",
    "                        \"machine_id\": mid,\n",
    "                        \"profit\": float(c[\"margin\"]),\n",
    "                        \"p_best\": float(c[\"p_best\"]),\n",
    "                        \"processing_hours\": need,\n",
    "                        \"profit_score\": float(c[\"profit_score\"]),\n",
    "                        \"base_confidence\": float(c[\"base_confidence\"]),\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "        assigned_df = pd.DataFrame(assigned)\n",
    "\n",
    "        # --- KPI computation (same as your compute_kpis function) ---\n",
    "        if assigned_df.shape[0] == 0:\n",
    "            kpis = dict(\n",
    "                expected_profit=0.0,\n",
    "                expected_ontime_deliveries=0.0,\n",
    "                factory_capacity_utilization=0.0,\n",
    "                expected_orders_completed=0,\n",
    "            )\n",
    "        else:\n",
    "            total_profit = assigned_df[\"profit\"].sum()\n",
    "            expected_ontime = assigned_df[\"p_best\"].sum()\n",
    "            total_used = assigned_df[\"processing_hours\"].sum()\n",
    "            total_capacity = mach[\"daily_capacity_hours\"].sum() * planning_days\n",
    "            factory_util = total_used / total_capacity\n",
    "            orders_completed = assigned_df[\"order_id\"].nunique()\n",
    "\n",
    "            kpis = dict(\n",
    "                expected_profit=float(total_profit),\n",
    "                expected_ontime_deliveries=float(expected_ontime),\n",
    "                factory_capacity_utilization=float(factory_util),\n",
    "                expected_orders_completed=int(orders_completed),\n",
    "            )\n",
    "\n",
    "        # --- Combine outputs ---\n",
    "        for k, v in kpis.items():\n",
    "            assigned_df[k] = v\n",
    "\n",
    "        # --- Commit if confirmed ---\n",
    "        if commit:\n",
    "            fq_output = f\"{self.catalog}.{self.schema}.assigned_scenario\"\n",
    "            spark.createDataFrame(assigned_df).write.mode(\"overwrite\").saveAsTable(fq_output)\n",
    "\n",
    "        return spark.createDataFrame(assigned_df)\n",
    "$$;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70a01d2-3037-41f5-9521-11556a255769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION zg.production_scheduling_demo.optimize_kpi_summary(\n",
    "  down_machines ARRAY<STRING>,\n",
    "  planning_days INT\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  expected_profit DOUBLE,\n",
    "  expected_ontime_deliveries DOUBLE,\n",
    "  factory_capacity_utilization DOUBLE,\n",
    "  expected_orders_completed INT\n",
    ")\n",
    "LANGUAGE PYTHON\n",
    "COMMENT \"Lightweight optimizer that runs greedy assignment in memory and returns only KPI summary for conversational previews.\"\n",
    "HANDLER \"KpiSummaryHandler\"\n",
    "AS $$\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class KpiSummaryHandler:\n",
    "    def __init__(self):\n",
    "        self.catalog = \"zg\"\n",
    "        self.schema = \"production_scheduling_demo\"\n",
    "\n",
    "    def __call__(self, down_machines, planning_days):\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        cand = spark.table(f\"{self.catalog}.{self.schema}.candidate_routes_scored\").toPandas()\n",
    "        mach = spark.table(f\"{self.catalog}.{self.schema}.machines_catalog\").toPandas()\n",
    "        down_machines = set(down_machines or [])\n",
    "\n",
    "        machine_caps = {\n",
    "            m[\"machine_id\"]: float(m[\"daily_capacity_hours\"] * planning_days)\n",
    "            for _, m in mach.iterrows()\n",
    "        }\n",
    "        for m in down_machines:\n",
    "            machine_caps[m] = 0.0\n",
    "\n",
    "        # Greedy assignment (simplified)\n",
    "        order_priority = (\n",
    "            cand.groupby(\"order_id\")\n",
    "                .agg(max_profit_score=(\"profit_score\", \"max\"))\n",
    "                .sort_values(by=\"max_profit_score\", ascending=False)\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "        cand_by_order = {\n",
    "            oid: g.sort_values(by=[\"profit_score\", \"processing_hours\"], ascending=[False, True])\n",
    "            for oid, g in cand.groupby(\"order_id\", sort=False)\n",
    "        }\n",
    "\n",
    "        assigned = []\n",
    "        for _, rowp in order_priority.iterrows():\n",
    "            oid = rowp[\"order_id\"]\n",
    "            for _, c in cand_by_order[oid].iterrows():\n",
    "                mid = c[\"machine_id\"]\n",
    "                if mid in down_machines:\n",
    "                    continue\n",
    "                need = float(c[\"processing_hours\"])\n",
    "                if machine_caps.get(mid, 0.0) >= need:\n",
    "                    machine_caps[mid] -= need\n",
    "                    assigned.append({\n",
    "                        \"profit\": float(c[\"margin\"]),\n",
    "                        \"p_best\": float(c[\"p_best\"]),\n",
    "                        \"processing_hours\": need,\n",
    "                        \"order_id\": oid\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "        assigned_df = pd.DataFrame(assigned)\n",
    "        if assigned_df.empty:\n",
    "            return spark.createDataFrame([{\n",
    "                \"expected_profit\": 0.0,\n",
    "                \"expected_ontime_deliveries\": 0.0,\n",
    "                \"factory_capacity_utilization\": 0.0,\n",
    "                \"expected_orders_completed\": 0\n",
    "            }])\n",
    "\n",
    "        total_profit = assigned_df[\"profit\"].sum()\n",
    "        expected_ontime = assigned_df[\"p_best\"].sum()\n",
    "        total_used = assigned_df[\"processing_hours\"].sum()\n",
    "        total_capacity = mach[\"daily_capacity_hours\"].sum() * planning_days\n",
    "        factory_util = total_used / total_capacity\n",
    "        orders_completed = assigned_df[\"order_id\"].nunique()\n",
    "\n",
    "        return spark.createDataFrame([{\n",
    "            \"expected_profit\": float(total_profit),\n",
    "            \"expected_ontime_deliveries\": float(expected_ontime),\n",
    "            \"factory_capacity_utilization\": float(factory_util),\n",
    "            \"expected_orders_completed\": int(orders_completed)\n",
    "        }])\n",
    "$$;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6492393898608566,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "06_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
